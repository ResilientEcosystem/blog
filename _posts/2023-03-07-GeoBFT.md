---
layout: article
title: GeoBFT in NexRes
author: Weijia Wang
tags: NexRes GeoBFT
aside:
    toc: true
article_header:
  type: overlay
  theme: dark
  background_color: '#000000'
  background_image:
    gradient: 'linear-gradient(135deg, rgba(0, 204, 154 , .2), rgba(51, 154, 154, .2))'
    src: /assets/images/resdb-gettingstarted/codecover.jpg

---

In this article, we present a comprehensive introduction into the **Geo-Scale Byzantine Fault-Tolerant consensus protocol (GeoBFT)** through the lens of the NexRes codebase. we will illustrate the algorithm of inter-cluster sharing and remote view-change of the protocol, and how to implement it on NexRes. We will also show the performance experiment results we got for GeoBFT on NexRes.

### The bottleneck in Geo-scale blockchains

The traditional blockchain protocols like PBFT[^1], rely on a single primary replica to coordinate all consensus decisions, and require a vast amount of global communication between all pairs of replicas, hence, they often attain low throughput, especially when the replicas are spread across a wide-area network, or geographically large distances.

As the following figure shows, the global message latencies are at least 33-270 times higher than local latencies, while the maximum throughput is 10-151 times lower, both implying that the communication between regions is much more costly than communications within regions.

<p>
    <img src="https://lh6.googleusercontent.com/3LdDm4lkyjiiKDbGJjS6jwx4L6RBOBnfxq2JB6c_cDOj6k-vHpSAcHJUWY-IVE2wKespom6o6LJ2xrU7kcgMh-i5BrFth_Hi6-wL9-bgNQSISuUvKbmhJJQN-pM4rPVpHpwAjha2uIJu2qk3ouLXtlXYaA=s2048" alt="Cover photo" style="width: 70%"/>
    <br>
    <em>Figure 1. Real-world inter- and intra-cluster communication costs in terms of the ping round-trip times (which determines latency) and bandwidth (which determines throughput). These measurements are taken in Google Cloud using clusters of n1 machines (replicas) that are deployed in six different regions.
    </em>
</p>

### Geo-Scale Byzantine Fault-Tolerant consensus protocol (GeoBFT)

Here we present GeoBFT that uses topological information to group all replicas in a single region into a single cluster[^2]. GeoBFT assigns each client to a single cluster, and it operates in rounds, in each round, every cluster will be able to propose a single client request for execution. Each round consists of the three steps sketched in Figure 2: local replication, global sharing, and ordering and execution, which we further detail next.

<p>
    <img src="https://lh6.googleusercontent.com/ps1-lJgiFjfr4SEFwnzL47N2XDhZpiwkRpaDxHz7iDWZz_h6sNYgwy0vPBoFVictXwIkpfwRXHy6tSIuSxlhpz2s-KjuXUXeWILaWY1VOlOVew284y5YNnjzBeKINFm-kZ1moElwL3br4Muzd7kKGRKl3Q=s2048" alt="Cover photo" style="width: 70%"/>
    <br>
    <em>Figure 2. Steps in a round of the GeoBFT protocol.
    </em>
</p>

#### Local Replication

In the first step of GeoBFT, each round starts with each cluster replicating a client request *T*, to do so, GeoBFT relies on PBFT. In addition, when each non-faulty replica commits the proposed request, it will also construct a *commit certificate* $\langle\mathcal{T}\rangle_{C}$ consist of the client request and **n-f > 2f** identical $\mathcal{COMMIT}$ messages for $\langle\mathcal{T}\rangle_{\mathcalP{C}}$ signed by distinct replicas. The following graph shows the normal-case workflow of round $\rho$ of PBFT within a cluster.

<p>
    <img src="{{ site.baseurl }}/assets/images/GeoBFT/normal-case of PBFT.png" alt="Cover photo" style="width: 70%"/>
    <br>
    <em>Figure 3. The normal-case working of round ρ of Pbft within a cluster C: a client c requests transaction T, the primary PC proposes this request to all local replicas, which prepare and commit this proposal, and, finally, all replicas can construct a commit certificate.
    </em>
</p>

#### Global Sharing

Once a cluster has completed local replication of a client request, it proceeds with the second step: sharing the client request with all other clusters. After $\mathcal{C}$ reaches local consensus on client request $\langle\mathcal{T}\rangle_{\mathcalP{C}}$ in round $\rho$—enabling construction of the commit certificate $[\langle\mathcal{T}\rangle_{\mathcalP{C}}, \rho]_{C}$ that proves local consensus—$\mathcal{C}$ needs to exchange this client request and the accompanying proof with all other clusters. The following graphs shows the global sharing process and the pseudo-code of this protocol, which we will illustrate next.

<p>
    <img src="{{ site.baseurl }}/assets/images/GeoBFT/Global Sharing.png" alt="Cover photo" style="width: 70%"/>
    <br>
    <em>Figure 4. The normal-case working of the global sharing protocol used by $\mathcal{C}_{1}$ to send $m = (\langle\mathcal{T}\rangle_{\mathcalP{C}}, [\langle\mathcal{T}\rangle_{\mathcalP{C}}, \rho]_{C_{1}}})$ to C2.
    </em>
</p>

<p>
    <img src="{{ site.baseurl }}/assets/images/GeoBFT/Global Sharing Pseudo Code.png" alt="Cover photo" style="width: 70%"/>
    <br>
    <em>Figure 5. The normal-case global sharing protocol used by $\mathcal{C}_{1}$ to send $m = (\langle\mathcal{T}\rangle_{\mathcalP{C}}, [\langle\mathcal{T}\rangle_{\mathcalP{C}}, \rho]_{C_{1}}})$ to C2.
    </em>
</p>

Let $m = (\langle\mathcal{T}\rangle_{\mathcalP{C}}, [\langle\mathcal{T}\rangle_{\mathcalP{C}}, \rho]_{C_{1}}})$ be the message that some replica in cluster $\mathcal{C_{1}}$ needs to send to some replicas $\mathcal{C_{2}}$.

In the global phase, the primary $\mathcal{P_{C_{1}}}$ sends $\mathcal{m}$ to $\mathcal{f+1}$ replicas in $\mathcal{C_{2}}$. In the local phase, each non-faulty replica $ \mathcal{R} \in nf(\mathcal{C_{2}})$ that receives a well-formed m forwards m to all replicas in its cluster $\mathcal{C_{2}}$.

There are two cases in which replicas in $\mathcal{P_{C_{2}}}$ do not receive $\mathcal{m}$ from $\mathcal{P_{C_{1}}}$: either $\mathcal{P_{C_{1}}}$ is faulty and did not send $\mathcal{m}$ to $\mathcal{f+1}$ replicas in $\mathcal{P_{C_{2}}}$, or communication is unreliable, and messages are delayed or lost. In both cases, non-faulty replicas in C2 initiate *remote view-change* to force primary replacement in $\mathcal{P_{C_{1}}}$.

#### Remote View-Change

To recover from any failures, we provide a remote view-change protocol. To simplify presentation, we focus on the case in which the primary of cluster $\mathcal{C_{1}}$ fails to send $m = (\langle\mathcal{T}\rangle_{\mathcalP{C}}, [\langle\mathcal{T}\rangle_{\mathcalP{C}}, \rho]_{C_{1}}})$ to replicas of $\mathcal{P_{C_{2}}}$. As the following graph describes, our remote view-change protocol consists of four phases, which we detail next.

- First, non-faulty replicas in cluster $\mathcal{C_{2}}$ detect the failure of the current primary $\mathcal{P_{C_{1}}}$ of $\mathcal{C_{1}}$ to send m. Note that although the replicas in $\mathcal{C_{2}}$ have no information about the contents of message m, they are awaiting arrival of a well-formed message m from $\mathcal{C_{1}}$ in round $\rho$.
- Second, the non-faulty replicas in $\mathcal{C_{2}}$ initiate agreement on failure detection.
- Third, after reaching agreement, the replicas in $\mathcal{C_{2}}$ send their request for a remote view-change to the replicas in $\mathcal{C_{1}}$ in a reliable manner.
- In the fourth and last phase, the non-faulty replicas in $\mathcal{C_{1}}$ trigger a local view-change, replace $\mathcal{P_{C_{1}}}$ , and instruct the new primary to resume global sharing with $\mathcal{C_{2}}$.

<p>
    <img src="{{ site.baseurl }}/assets/images/GeoBFT/Remote View Change.png" alt="Cover photo" style="width: 70%"/>
    <br>
    <em>Figure 6. The remote view-change protocol of GeoBFT. This protocol is triggered when a cluster $\mathcal{C_{2}}$ ∈ S expects a message from $\mathcal{C_{1}}$ ∈ S, but does not receive this message in time.
    </em>
</p>

#### Ordering and Execution

Once replicas of a cluster have chosen a client request for execution and have received all client requests chosen by other clusters, the final step is ordering and executing these client requests. In specific, in round $\rho$, any non-faulty replica that has valid requests from all clusters can move ahead and execute these requests. To put these client requests in a unique order, execute them, and inform the clients of the outcome, GeoBFT simply uses a pre-defined ordering on the clusters. For example, each replica executes the transactions in the order $[\mathcal{T_{1}}, . . . , T_{z}}]$. Once the execution is complete, each replica $R \in \mathcal{C_{i}}, 1 \leq i \leq z$, informs the client $\mathcal{c_{i}}$ of any outcome.



### Implementation

Now we will give a brief overview of the implementation of GeoBFT protocol in NexRes. To implement GeoBFT on NexRes, we will need to add a few modules: *Local Executor, Global Executor, GeoBFT Consensus Service*, and also adjust the *Configuration*, which we will introduce next. Below is the normal-case working graph of GeoBFT on NexRes.

<p>
    <img src="{{ site.baseurl }}/assets/images/GeoBFT/GeoBFT_1.png" alt="Cover photo" style="width: 100%"/>
    <br>
    <em>Figure 7. The normal-case working of GeoBFT on NexRes 
    </em>
</p>


#### Configuration

To implement GeoBFT on NexRes, we need to first add a few fields in the configuration file, which specify the timeout for remote view-change, and the flag that indicates whether the local view-change is enabled.

```
{
  region: [
    {
      replicaInfo: {
        id:
        ip:
        port:
      },
      regionId: 1
    }
  ],
  selfRegionId: 1,
  rvc_timeout_ms: 60000,	 # Timeout for Remote View-Change
  enable_viewchange: true	 # View-Change Flag
}
```

#### Local Executor

After committing a local client request at the end of PBFT, the local executor will construct the *commit certificate* which is of *TYPE_GEO_REQUEST*, here we call it *geo_request*. The primary replica will be responsible for sending out the *geo_request* to every other cluster in the system. Specifically, for each round, the same *geo_request* will only needs to be sent to *f+1* replicas in each cluster. 

```c++
void GeoTransactionExecutor::SendBatchGeoMessage(
    const std::vector<std::unique_ptr<Request>>& batch_geo_request) {
    // Construct geo_request
    std::unique_ptr<Request> geo_request = resdb::NewRequest(
      Request::TYPE_GEO_REQUEST, Request(), config_.GetSelfInfo().id(),
      config_.GetConfigData().self_region_id());
    ...	// set sequence number, proxy id, hash value, data in geo_request
	// Only for primary node, send out geo_request to other regions.
    if (config_.GetSelfInfo().id() == system_info_->GetPrimaryId()) {	
        for (const auto& region : config_data.region()) {
            if (region.region_id() == config_data.self_region_id())	continue;
            int max_faulty = (region.replica_info_size() - 1) / 3;		// maximum number of faulty replicas in this region
            int num_request_sent = 0;
            for (const auto& replica : region.replica_info()) {
                if (num_request_sent > max_faulty) {
                    break;
                }
                LOG(ERROR) << "send batch_geo_request to node " << replica.id();
                int ret = replica_client_->SendBatchMessage(batch_geo_request, replica);	// send to f + 1 replicas in the region
                if (ret >= 0) {
                    num_request_sent++;
                }
            }
        }
    }
}
```

#### Global Executor

After receiving a *geo_request* from another region, push it into *order_queue_*, which is storing the transactions that are not yet ordered. The global executor will constantly pop out *geo_request* from *order_queue_*, and add it to *execute_map_*, which is storing the transactions that are waiting to be executed by order. Then, the global executor will find the next transaction to execute in the *execute_map_* according to current round $\rho$. We use an thread to keep this ordering and execution process going.

```c++
std::map<std::pair<uint64_t, int>, std::unique_ptr<Request>> execute_map_;
std::thread order_thread_ = std::thread(&GeoGlobalExecutor::OrderRound, this);

int GeoGlobalExecutor::OrderGeoRequest(std::unique_ptr<Request> request) {
    order_queue_.Push(std::move(request));
	return 0;
}

void GeoGlobalExecutor::AddData() {
	auto request = order_queue_.Pop();	// Pop geo_request from order_queue_
	if (request == nullptr) {
		return;
  	}
    uint64_t seq_num = request->seq();
    int region_id = request->region_info().region_id();
	execute_map_[std::make_pair(seq_num, region_id)] = std::move(request);	// add geo_request to execute_map_
}

void GeoGlobalExecutor::OrderRound() {
	while (!IsStop()) {
    	AddData();	// Pop geo_request from order_queue_, add it to execute_map_
    	while (!IsStop()) {
			std::unique_ptr<Request> seq_map = GetNextMap();	// Get next geo_request to execute in the execute_map_
            if (seq_map == nullptr) {
                break;
            }
            Execute(std::move(seq_map));	// Execute the geo_request and reset the timer for remote view change.
    	}
  	}
}
```

The global executor also has $\mathcal{n-1}$ timers to trigger remote view-change in $\mathcal{n-1}$ other regions. Let $\mathcal{R} \in \mathcal{C_{1}}$ be the replica we are looking at. Upon receiving a geo_request from another region, the global executor will need to reset the timer for that region. When the timeout function is triggered, $\mathcal{R}$ will broadcast a *drvc_request* locally. After receiving $\mathcal{f+1}$ distinct *drvc_request* from other replicas in local cluster, $\mathcal{R}$ will also start to broadcast a *drvc_request* locally. After receiving $\mathcal{n-f}$ distinct *drvc_request* from other replicas in local cluster, $\mathcal{R}$ will start to send a new type of request-*rvc_request* to $\mathcal{Q} \in \mathcal{C_{2}}$ with $id(\mathcal{Q})=id(\mathcal{R})$, with $\mathcal{C}_{2}$ being the target cluster of this remote view-change.

```c++
void GeoGlobalExecutor::Execute(std::unique_ptr<Request> request) {
    ...
    // reset the timer for the region which the request came from.
    int region_id = request->region_info().region_id();
    if (next_region_ == 1){
      TryResetTimer(next_seq_, region_id);	
    }
    else{
      TryResetTimer(next_seq_+1, region_id);
    }
}

// detect failure of region rvc_target_region in round ρ, trigger RVC
void GeoGlobalExecutor::TriggerRVCLocalBroadcast(int rvc_target_region) {
    ...
    // broadcast drvc_request locally
    std::unique_ptr<Request> drvc_request = resdb::NewRequest(
        Request::TYPE_DRVC, Request(), config_.GetSelfInfo().id(),
        config_.GetConfigData().self_region_id());
    drvc_request->set_current_executed_seq(missing_seq);
    drvc_request->set_rvc_target_region(rvc_target_region);
    replica_client_->BroadCast(*drvc_request);
}
```

#### GeoBFT Consensus Service

In order to process those types of messages we mentioned (*geo_request, drvc_request, rvc_request*) we need to implement the *GeoBFT Consensus Service* as the entry for the process of those requests.

```c++
int ConsensusServiceGeoPBFT::ConsensusCommit(std::unique_ptr<Context> context,
                                             std::unique_ptr<Request> request) {
  switch (request->type()) {
    case Request::TYPE_GEO_REQUEST:
      return commitment_->GeoProcessCcm(std::move(context), std::move(request));		// Process geo_request
    case Request::TYPE_DRVC:
      return commitment_->ProcessDRVC(std::move(context), std::move(request));			// Process drvc_request
    case Request::TYPE_RVC:
      return commitment_->ProcessRVC(std::move(context), std::move(request));			// Process rvc_request
  }
  return ConsensusServicePBFT::ConsensusCommit(std::move(context), std::move(request));	// Norcal-case PBFT processing
}

int GeoPBFTCommitment::GeoProcessCcm(std::unique_ptr<Context> context,
                                     std::unique_ptr<Request> request) {
    ...	// filter of duplicate message
    // if the request comes from another region, do local broadcast
    if (sender_region_id != self_region_id) {
        std::unique_ptr<Request> broadcast_geo_req =
            resdb::NewRequest(Request::TYPE_GEO_REQUEST, *request,
                              config_.GetSelfInfo().id(), sender_region_id);
        replica_client_->BroadCast(*broadcast_geo_req);
    }
    // push the request into order_queue_
    return global_executor_->OrderGeoRequest(std::move(request));
}

int GeoPBFTCommitment::ProcessDRVC(std::unique_ptr<Context> context,
                                   std::unique_ptr<Request> request) {
    int missing_seq = request->current_executed_seq();
    int rvc_target_region = request->rvc_target_region();
    int drvc_sender_id = request->sender_id();
    ...	// filter of duplicate messages
    // when receives f+1 DRVC request: detect failure of region C in round
    // ρ broadcast DRvc(C, ρ) to all replicas in local region.
    int num_replicas = config_.GetReplicaNum();
    int max_faulty = (num_replicas - 1) / 3;
    if (drvc_map_[std::make_pair(rvc_target_region, missing_seq)].size() == max_faulty + 1) {
		global_executor_->TriggerRVCLocalBroadcast(rvc_target_region);
    }
    // when receives DRvc(C1, ρ) from n − f replicas, send Rvc(C1, ρ)_R to Q ∈ C1, id(R) = id(Q).
    if (drvc_map_[std::make_pair(rvc_target_region, missing_seq)].size() == num_replicas - max_faulty) {
        std::unique_ptr<Request> rvc_request = resdb::NewRequest(
            Request::TYPE_RVC, Request(), config_.GetSelfInfo().id(),
            config_.GetConfigData().self_region_id());
        rvc_request->set_current_executed_seq(missing_seq);
        rvc_request->set_rvc_target_region(rvc_target_region);
        ...	// find target_replica Q
        replica_client_->SendMessage(*rvc_request, target_replica);
    } 
}

int GeoPBFTCommitment::ProcessRVC(std::unique_ptr<Context> context,
                                  std::unique_ptr<Request> request) {
    uint64_t missing_seq = request->current_executed_seq();
	int sender_id = request->sender_id();
	int max_faulty = (config_.GetReplicaNum() - 1) / 3;
    ...	// filter of duplicate messages
    // If received f+1 idendical rvc from distinct replicas in C2, trigger local view change.
    if (rvc_map_[missing_seq].size() == max_faulty + 1) {
		viewchange_manager_->RunTimeoutFunc(missing_seq);
	}
    // Broadcast rvc locally.
    if (request->region_info().region_id() != config_.GetConfigData().self_region_id()) {
        std::unique_ptr<Request> rvc_request = resdb::NewRequest(
            Request::TYPE_RVC, Request(), config_.GetSelfInfo().id(),
            config_.GetConfigData().self_region_id());
        rvc_request->set_current_executed_seq(missing_seq);
        replica_client_->BroadCast(*rvc_request);
    }
}
```



### Experiments and Results

#### Experiment Setup

We ran all the experiments on AWS using:

- t3.2xlarge
- 8 vCPU
- 64GB of RAM

Nexres ran on Ubuntu, 20.04 LTS, amd64 focal image build on 2022-09-14.

#### Throughput and Latency Performance

The first experiment is designed to test the maximum throughput and latency of GeoBFT and compare the result with PBFT performance under the same configuration. The throughput

<p>
    <img src="{{ site.baseurl }}/assets/images/GeoBFT/Performance Tpt.png" alt="Cover photo" style="width: 60%"/>
    <img src="{{ site.baseurl }}/assets/images/GeoBFT/Performance Latency.png" alt="Cover photo" style="width: 60%"/>
    <br>
    <em>Figure 8. Throughput and latency as a function of the number of replicas.
    </em>
</p>



### References

[^1]: **Castro, Miguel, and Barbara Liskov. "*Practical byzantine fault tolerance.*" OsDI. Vol. 99. No. 1999. 1999.** 
[^2]: **Suyash Gupta, Sajjad Rahnama, Jelle Hellings, and Mohammad Sadoghi. "*ResilientDB: Global Scale Resilient Blockchain Fabric*" URL: https://arxiv.org/abs/2002.00160 **